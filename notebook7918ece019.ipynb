{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"df1c7f6fa1934123b1007b39f8b43b3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cbfb237086044d391f6dacdb46d5722","IPY_MODEL_2ed38e8287854835a7a2190b01b586c1","IPY_MODEL_e7da851b4c3148cb806ec318d696ca83"],"layout":"IPY_MODEL_e56d5aa5dcda4b0ab20a966a0432f632"}},"0cbfb237086044d391f6dacdb46d5722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c11609f79c48d98a9884496d849fdf","placeholder":"​","style":"IPY_MODEL_0c78c32ce6104d0cbec9c5e7d375fa5e","value":"Filter: 100%"}},"2ed38e8287854835a7a2190b01b586c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b399655347e460abf84fd7de6975a32","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9595827822d94a62a3fc9ae71e1ca37f","value":1000}},"e7da851b4c3148cb806ec318d696ca83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19ccf0a4a8d64b9587f53db344a7a362","placeholder":"​","style":"IPY_MODEL_7b26473d9636406f9f0977beff37753f","value":" 1000/1000 [00:00&lt;00:00, 10325.91 examples/s]"}},"e56d5aa5dcda4b0ab20a966a0432f632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c11609f79c48d98a9884496d849fdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c78c32ce6104d0cbec9c5e7d375fa5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b399655347e460abf84fd7de6975a32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9595827822d94a62a3fc9ae71e1ca37f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19ccf0a4a8d64b9587f53db344a7a362":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b26473d9636406f9f0977beff37753f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c62205f4e49d4ff78811540349b60cf1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f2bff7ab8bd4a68b20639d05cddc35b","IPY_MODEL_89c4439cc3554342841d6e60802948db","IPY_MODEL_e4b7640e65df44af8c4024114144821f"],"layout":"IPY_MODEL_7b97b7fddc674028a097a8e505216ce9"}},"3f2bff7ab8bd4a68b20639d05cddc35b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_127d3c2d723244d8974a526e75265cb7","placeholder":"​","style":"IPY_MODEL_b1f14efb56bc4c4691c34991c1b2339c","value":"Map: 100%"}},"89c4439cc3554342841d6e60802948db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b613fac68bd44e3681f52aad83c343db","max":771,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1159febb5d9444fd8442260348ddeaff","value":771}},"e4b7640e65df44af8c4024114144821f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb210b7b6f2f49a1b9609eddd094eed6","placeholder":"​","style":"IPY_MODEL_d90dbc148b9040f2b5bfc2a23ec79d37","value":" 771/771 [00:00&lt;00:00, 4018.62 examples/s]"}},"7b97b7fddc674028a097a8e505216ce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"127d3c2d723244d8974a526e75265cb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1f14efb56bc4c4691c34991c1b2339c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b613fac68bd44e3681f52aad83c343db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1159febb5d9444fd8442260348ddeaff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb210b7b6f2f49a1b9609eddd094eed6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90dbc148b9040f2b5bfc2a23ec79d37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9416166b2bd14d698b250cf154336672":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb9bf0c1955044128c923b473b70fc91","IPY_MODEL_d15aa7c477e54a29bda5d875eb115143","IPY_MODEL_b5b12ef59f354f95bcf49484ffeeaa4d"],"layout":"IPY_MODEL_bfba214300ec45e196a42ce79b8ff475"}},"eb9bf0c1955044128c923b473b70fc91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba02ec1d414a497f92f41bd7a7e51998","placeholder":"​","style":"IPY_MODEL_722df1bc8de34c24a894e29be41c8413","value":"Map: 100%"}},"d15aa7c477e54a29bda5d875eb115143":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_331b27be3df24abe9a474e9415db77a2","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6935f32da5a4c169b4c2ba0adc56418","value":1000}},"b5b12ef59f354f95bcf49484ffeeaa4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_596eeffc44b5412cbb362ee840f8bd32","placeholder":"​","style":"IPY_MODEL_8ec635451a5044319a42b09ef6125ea1","value":" 1000/1000 [00:00&lt;00:00, 2136.21 examples/s]"}},"bfba214300ec45e196a42ce79b8ff475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba02ec1d414a497f92f41bd7a7e51998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"722df1bc8de34c24a894e29be41c8413":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"331b27be3df24abe9a474e9415db77a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6935f32da5a4c169b4c2ba0adc56418":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"596eeffc44b5412cbb362ee840f8bd32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ec635451a5044319a42b09ef6125ea1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e0fb78a16ac4ba1b0ec9024c7142a87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a50a6697b5334478888a90aaea2e992d","IPY_MODEL_5eadbc9e8eb34bb0b6538368544d8b7b","IPY_MODEL_2e3d6d8053de4e2da0c48ecbd7be0ca5"],"layout":"IPY_MODEL_8310127d6d2b44f680a4492775f51bdb"}},"a50a6697b5334478888a90aaea2e992d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b4a513f3bfa46e29f62bb173d336002","placeholder":"​","style":"IPY_MODEL_500b494a87fd4975bfabfd4fa489ef9e","value":"Map: 100%"}},"5eadbc9e8eb34bb0b6538368544d8b7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_841b74a394c74e64852ac9837fc412d5","max":2713,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6052d4b351e04d409890247cd3a6f2f0","value":2713}},"2e3d6d8053de4e2da0c48ecbd7be0ca5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb579701ad344a199f98544de11ec893","placeholder":"​","style":"IPY_MODEL_91fa95830fc74ba4a46d4be2ea81d6fa","value":" 2713/2713 [00:02&lt;00:00, 975.86 examples/s]"}},"8310127d6d2b44f680a4492775f51bdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b4a513f3bfa46e29f62bb173d336002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"500b494a87fd4975bfabfd4fa489ef9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"841b74a394c74e64852ac9837fc412d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6052d4b351e04d409890247cd3a6f2f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb579701ad344a199f98544de11ec893":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91fa95830fc74ba4a46d4be2ea81d6fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"id":"RA8NVaC4wX3p"}},{"cell_type":"code","source":"!pip install transformers datasets evaluate","metadata":{"id":"BW1qhuHXzjMW","outputId":"4f6e70a8-07a9-4670-99b0-98163ee96088","execution":{"iopub.status.busy":"2024-11-02T23:29:21.479095Z","iopub.execute_input":"2024-11-02T23:29:21.479394Z","iopub.status.idle":"2024-11-02T23:29:35.160894Z","shell.execute_reply.started":"2024-11-02T23:29:21.47936Z","shell.execute_reply":"2024-11-02T23:29:35.159633Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n\n# model_name = \"nur-dev/roberta-kaz-large\"\n# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel_checkpoint = \"nur-dev/roberta-kaz-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"2OuRTALKrdt4","outputId":"eea149c5-5e3a-4d22-9689-d53edfcc88ec","execution":{"iopub.status.busy":"2024-11-02T23:50:09.910842Z","iopub.execute_input":"2024-11-02T23:50:09.91139Z","iopub.status.idle":"2024-11-02T23:50:10.224013Z","shell.execute_reply.started":"2024-11-02T23:50:09.911334Z","shell.execute_reply":"2024-11-02T23:50:10.222938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntrain_ds = load_dataset(\"Kyrmasch/sKQuAD\")\n\ntest_ds = load_dataset(\"issai/kazqad\")\ntest_ds['test'] = test_ds['test'].select(range(1000))","metadata":{"id":"ISoq3P_2r5rr","execution":{"iopub.status.busy":"2024-11-02T23:50:11.008702Z","iopub.execute_input":"2024-11-02T23:50:11.009112Z","iopub.status.idle":"2024-11-02T23:50:12.846066Z","shell.execute_reply.started":"2024-11-02T23:50:11.009071Z","shell.execute_reply":"2024-11-02T23:50:12.845064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def has_answer(example):\n    answer = example['answer']\n    context = example['context']\n    answer_start = context.lower().find(answer.lower())\n    if answer_start == -1:\n        return False\n    return True\n\ntrain_ds['train'] = train_ds['train'].filter(has_answer)","metadata":{"id":"6kClIqtLtib8","outputId":"ad6a9c61-9a58-483e-a30e-c92f114f3cb9","execution":{"iopub.status.busy":"2024-11-02T23:50:12.884106Z","iopub.execute_input":"2024-11-02T23:50:12.884423Z","iopub.status.idle":"2024-11-02T23:50:12.893848Z","shell.execute_reply.started":"2024-11-02T23:50:12.88439Z","shell.execute_reply":"2024-11-02T23:50:12.893039Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_answer_start(example):\n    answer = example['answer']\n    context = example['context']\n    answer_start = context.lower().find(answer.lower())\n    example['answers'] = {'text': [answer],\n                          'answer_start': [answer_start]}\n    return example\n\ntrain_ds['train'] = train_ds['train'].map(add_answer_start)","metadata":{"id":"7LMihZreueAB","outputId":"f2c18e8b-14b4-4438-a97f-cf9ef478a280","execution":{"iopub.status.busy":"2024-11-02T23:50:14.653471Z","iopub.execute_input":"2024-11-02T23:50:14.654426Z","iopub.status.idle":"2024-11-02T23:50:14.665413Z","shell.execute_reply.started":"2024-11-02T23:50:14.654367Z","shell.execute_reply":"2024-11-02T23:50:14.664475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_length = 384 # hyperparameter\nstride = 128 # hyperparameter\n\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"id":"T0RE7Xq6u9HB","execution":{"iopub.status.busy":"2024-11-02T23:50:16.195445Z","iopub.execute_input":"2024-11-02T23:50:16.196395Z","iopub.status.idle":"2024-11-02T23:50:16.207906Z","shell.execute_reply.started":"2024-11-02T23:50:16.196323Z","shell.execute_reply":"2024-11-02T23:50:16.206901Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_ds['train'].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=train_ds[\"train\"].column_names,\n)\nlen(train_ds['train']), len(train_dataset)","metadata":{"id":"DVpBwEshvN-v","execution":{"iopub.status.busy":"2024-11-02T23:50:18.048354Z","iopub.execute_input":"2024-11-02T23:50:18.048774Z","iopub.status.idle":"2024-11-02T23:50:18.09833Z","shell.execute_reply.started":"2024-11-02T23:50:18.048735Z","shell.execute_reply":"2024-11-02T23:50:18.097425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:50:19.109013Z","iopub.execute_input":"2024-11-02T23:50:19.109386Z","iopub.status.idle":"2024-11-02T23:50:19.118436Z","shell.execute_reply.started":"2024-11-02T23:50:19.109348Z","shell.execute_reply":"2024-11-02T23:50:19.117544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"validation_dataset = test_ds['test'].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=test_ds['test'].column_names\n)\nlen(test_ds['test']), len(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:50:19.970208Z","iopub.execute_input":"2024-11-02T23:50:19.971116Z","iopub.status.idle":"2024-11-02T23:50:20.927422Z","shell.execute_reply.started":"2024-11-02T23:50:19.971068Z","shell.execute_reply":"2024-11-02T23:50:20.926546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:50:20.929162Z","iopub.execute_input":"2024-11-02T23:50:20.929597Z","iopub.status.idle":"2024-11-02T23:50:21.812211Z","shell.execute_reply.started":"2024-11-02T23:50:20.929549Z","shell.execute_reply":"2024-11-02T23:50:21.811438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()\n\nargs = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=20,\n#     label_names=[\"start_positions\", \"end_positions\"],\n#     per_device_train_batch_size=1,\n#     per_device_eval_batch_size=1,\n    learning_rate=1e-5, # hyperparameter\n    num_train_epochs=50, # hyperparameter\n    weight_decay=0.01, # hyperparameter\n    fp16=True, # hyperparameter\n    push_to_hub=False,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:50:24.548264Z","iopub.execute_input":"2024-11-02T23:50:24.548681Z","iopub.status.idle":"2024-11-02T23:50:24.588194Z","shell.execute_reply.started":"2024-11-02T23:50:24.548643Z","shell.execute_reply":"2024-11-02T23:50:24.587233Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n#     data_collator=data_collator,\n)\n# trainer.can_return_loss = True\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:50:27.392941Z","iopub.execute_input":"2024-11-02T23:50:27.393315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport evaluate\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    n_best = 20\n    max_answer_length = 30\n    metric = evaluate.load(\"squad\")\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:45:31.116375Z","iopub.execute_input":"2024-11-02T23:45:31.117209Z","iopub.status.idle":"2024-11-02T23:45:31.129832Z","shell.execute_reply.started":"2024-11-02T23:45:31.117161Z","shell.execute_reply":"2024-11-02T23:45:31.128687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import collections\nimport numpy as np\n\npredictions, _, _ = trainer.predict(validation_dataset)\nstart_logits, end_logits = predictions\ncompute_metrics(start_logits, end_logits, validation_dataset, test_ds[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-02T23:45:31.854077Z","iopub.execute_input":"2024-11-02T23:45:31.854451Z","iopub.status.idle":"2024-11-02T23:46:16.675618Z","shell.execute_reply.started":"2024-11-02T23:45:31.854413Z","shell.execute_reply":"2024-11-02T23:46:16.674645Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# end of the notebook","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForQuestionAnswering\n\neval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\neval_set_for_model.set_format(\"torch\")\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nbatch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n    device\n)\n\nwith torch.no_grad():\n    outputs = trained_model(**batch)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answers = train_ds[\"answer\"]\ncontexts = train_ds[\"context\"]\nstart_positions = []\nend_positions = []\n\ncount = 0\n\nfor i, offset in enumerate(inputs[\"offset_mapping\"]):\n    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n    if i > 999:\n      continue\n    answer = answers[i].lower()\n    start_char = contexts[i].lower().find(answer)\n    if start_char == -1:\n      count += 1\n    end_char = start_char + len(answer)\n    sequence_ids = inputs.sequence_ids(i)\n\n    # Find the start and end of the context\n    idx = 0\n    while sequence_ids[idx] != 1:\n        idx += 1\n    context_start = idx\n    while sequence_ids[idx] == 1:\n        idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label is (0, 0)\n    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nstart_positions, end_positions, count","metadata":{"id":"9Kbf_-qzmitI","outputId":"bcf72732-7c74-442d-e75c-68c5b93e77d9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = 8\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\nanswer = answers[sample_idx]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")","metadata":{"id":"CFMCRKstpQWV","outputId":"388eb20f-bc94-4bff-e6e6-1271db4512e1"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n\nmodel_name = \"nur-dev/roberta-kaz-large\"\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef preprocess_train_data(example):\n    inputs = tokenizer(\n        example['question'],\n        example['context'],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n    start_positions = []\n    end_positions = []\n    for i in range(len(example['answer'])):\n        answer = example['answer'][i].lower()\n        start_position = example['context'][i].lower().find(answer)\n        if start_position == -1:\n            end_position = -1\n        else:\n            end_position = start_position + len(answer)\n        start_positions.append(start_position)\n        end_positions.append(end_position)\n    inputs['start_positions'] = start_positions\n    inputs['end_positions'] = end_positions\n    return inputs\n\ndef preprocess_test_data(example):\n    inputs = tokenizer(\n        example['question'],\n        example['context'],\n        max_length=512,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\",\n    )\n    start_positions = []\n    end_positions = []\n    for i in range(len(example['answers'])):\n        answer = example['answers'][i]['text'][0]\n        start_position = example['answers'][i]['answer_start'][0]\n        end_position = start_position + len(answer)\n        start_positions.append(start_position)\n        end_positions.append(end_position)\n    inputs['start_positions'] = start_positions\n    inputs['end_positions'] = end_positions\n    return inputs\n\n# Apply preprocessing to the entire dataset\ntrain_ds = train_ds.map(preprocess_train_data, batched=True)\ntest_ds = test_ds.map(preprocess_test_data, batched=True)\n\n# ds = ds.map(preprocess_function, batched=True, remove_columns=ds[\"train\"].column_names)","metadata":{"id":"E_WDOL3ZJZxx","outputId":"d0f181d1-26f6-4850-d303-c1deb6d002ce"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DefaultDataCollator\n\ndata_collator = DefaultDataCollator()\n\ntraining_args = TrainingArguments(\n    output_dir=\"my_awesome_qa_model\",\n    eval_strategy=\"epoch\",\n    learning_rate=2e-5, # increase to 2e-4, 2e-3\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3, # increase up to 5, 10, 15, 20\n    weight_decay=0.01, # if overfitting is detected, increase to 0.03, 0.05, 0.07.\n    push_to_hub=False, # is there is dropout parameter, then use it (in case of overfitting)\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"id":"bfwujMnvQQRz","outputId":"fe7845bc-4efa-45af-c6be-dac06618eac5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"id":"MSZhPUgwaDkD","outputId":"f9095acd-670d-448d-f47f-0890ffd33086"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\nfrom evaluate import load\nfrom tqdm import tqdm\n\n# Load the trained model and tokenizer\nmodel_name = \"my_awesome_qa_model/checkpoint-750/\"  # Adjust this to your model's save path\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(\"nur-dev/roberta-kaz-large\")\n\n# Load the test dataset (ensure it is in the correct format)\ntest_ds = load_dataset(\"issai/kazqad\", \"kazqad\", split=\"test[100:150]\")  # Adjust this as needed\n\n# Load metrics\nf1_metric = load(\"f1\")\nem_metric = load(\"exact_match\")\n\ndef evaluate_model(dataset):\n    model.eval()  # Set the model to evaluation mode\n    all_start_logits = []\n    all_end_logits = []\n\n    with torch.no_grad():\n        for example in tqdm(dataset):\n            # Tokenize the input\n            inputs = tokenizer(\n                example[\"question\"],\n                example[\"context\"],\n                return_tensors=\"pt\",\n                max_length=512,\n                truncation=True,\n                padding=\"max_length\"\n            )\n\n            # Move inputs to the same device as the model\n            inputs = {key: val.to(model.device) for key, val in inputs.items()}\n\n            # Get model predictions\n            outputs = model(**inputs)\n            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n\n            all_start_logits.append(start_logits.cpu().numpy())\n            all_end_logits.append(end_logits.cpu().numpy())\n\n    return np.concatenate(all_start_logits), np.concatenate(all_end_logits)\n\n# Get model predictions\nstart_logits, end_logits = evaluate_model(test_ds)\n\n","metadata":{"id":"EGOoY9eUUbW7","outputId":"dc880937-a802-4a9b-f270-a2e3d1ab0f25"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Function to calculate metrics\ndef compute_metrics(pred_start_logits, pred_end_logits):\n    pred_start_ids = np.argmax(pred_start_logits, axis=1)\n    pred_end_ids = np.argmax(pred_end_logits, axis=1)\n\n    # Initialize the lists for the answers\n    answers = []\n    references = [item[\"answers\"][\"text\"][0] for item in test_ds]  # Adjust this line if necessary\n\n    for idx, (start, end) in enumerate(zip(pred_start_ids, pred_end_ids)):\n        if start <= end:  # Ensure valid answer span\n            pred_text = test_ds[idx][\"context\"][start:end + 1]  # Extract answer from context\n            answers.append(pred_text)\n        else:\n            answers.append(\"\")  # No valid answer\n\n    # Calculate the F1 and EM scores\n    for idx, answer in enumerate(answers):\n        true_answer = references[idx]\n        print(f\"({true_answer} - {answer})\")\n        # f1_metric.add_batch(predictions=[answer], references=[true_answer])\n        # em_metric.add_batch(predictions=[answer], references=[true_answer])\n\n    f1_score = f1_metric.compute()\n    em_score = em_metric.compute()\n\n    return {\n        \"f1\": f1_score[\"f1\"],\n        \"exact_match\": em_score[\"exact_match\"],\n    }\n\n# Calculate and print the evaluation metrics\nmetrics = compute_metrics(start_logits, end_logits)\nprint(f\"F1 Score: {metrics['f1']:.2f}\")\nprint(f\"Exact Match Score: {metrics['exact_match']:.2f}\")\n\n# from datasets import load_dataset, DatasetDict\n# from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer\n# import gc\n# import torch\n\n# # Проверяем, доступен ли GPU и используем его, если возможно\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(\"Using device:\", device)\n\n# # Загрузка модели и токенизатора\n# model_name = \"nur-dev/roberta-kaz-large\"\n# model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Загрузка полного набора данных\n# dataset = load_dataset(\"issai/kazqad\")\n\n# # **Выбор первых 100 записей из тренировочного и валидационного наборов**\n# small_dataset = DatasetDict({\n#     'train': dataset['train'].select(range(100)),\n#     'validation': dataset['validation'].select(range(100))\n# })\n\n# # Функция предобработки данных с проверкой на корректность позиций\n# def preprocess_data(batch):\n#     inputs = tokenizer(\n#         batch['question'],\n#         batch['context'],\n#         max_length=256,  # Увеличили максимальную длину для полного контекста\n#         truncation=True,\n#         padding=\"max_length\",\n#         return_offsets_mapping=True  # Добавили для получения смещений\n#     )\n\n#     start_positions = []\n#     end_positions = []\n\n#     for i in range(len(batch['answers'])):\n#         answer = batch['answers'][i]\n#         if answer['answer_start'] and answer['text']:\n#             # Получаем позиции ответа в виде символов\n#             start_char = answer['answer_start'][0]\n#             end_char = start_char + len(answer['text'][0])\n\n#             # Получаем offset_mapping для текущего примера\n#             offsets = inputs['offset_mapping'][i]\n\n#             # Инициализируем токеновые позиции\n#             start_token = None\n#             end_token = None\n\n#             for idx, (offset_start, offset_end) in enumerate(offsets):\n#                 if offset_start <= start_char < offset_end:\n#                     start_token = idx\n#                 if offset_start < end_char <= offset_end:\n#                     end_token = idx\n#                 if start_token is not None and end_token is not None:\n#                     break\n\n#             # Если не нашли токен, устанавливаем в 0\n#             if start_token is None:\n#                 start_token = 0\n#             if end_token is None:\n#                 end_token = 0\n#         else:\n#             start_token = 0\n#             end_token = 0\n\n#         start_positions.append(start_token)\n#         end_positions.append(end_token)\n\n#     # Добавляем позиции в inputs\n#     inputs['start_positions'] = start_positions\n#     inputs['end_positions'] = end_positions\n\n#     # Удаляем offset_mapping, так как он не нужен модели\n#     inputs.pop(\"offset_mapping\")\n\n#     return inputs\n\n# # Применение предобработки к уменьшенному набору данных\n# tokenized_dataset = small_dataset.map(\n#     preprocess_data,\n#     batched=True,\n#     remove_columns=small_dataset[\"train\"].column_names,\n# )\n\n# # Параметры обучения\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",\n#     evaluation_strategy=\"epoch\",\n#     logging_strategy=\"steps\",\n#     logging_steps=10,\n#     learning_rate=3e-5,\n#     per_device_train_batch_size=2,  # Уменьшенный размер батча из-за малого объема данных\n#     per_device_eval_batch_size=2,\n#     num_train_epochs=5,\n#     weight_decay=0.01,\n#     report_to=\"none\"\n# )\n\n# # Очистка памяти перед началом обучения\n# torch.cuda.empty_cache()\n# gc.collect()\n\n# # Настройка Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=tokenized_dataset[\"train\"],\n#     eval_dataset=tokenized_dataset[\"validation\"]\n# )\n\n# # Запуск тренировки\n# trainer.train()\n\n# # Сохранение модели в конце тренировки\n# trainer.save_model(\"./results\")  # Сохраняем только итоговую модель","metadata":{"id":"dQE8nwLEmm6f","outputId":"b903d211-a0a8-4ce6-d1dc-0277e8a57649"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def ask_question(question, context):\n    # Токенизация вопроса и контекста\n    inputs = tokenizer(\n        question,\n        context,\n        max_length=256,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n\n    # Переносим входные данные на устройство модели\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n\n    # Применяем модель для предсказания\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Получаем начальные и конечные позиции ответа\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n\n    # Находим наилучшие индексы для начала и конца ответа\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n\n    # Извлекаем ответ из токенизированного контекста\n    all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n    answer = tokenizer.convert_tokens_to_string(all_tokens[start_index:end_index + 1])\n\n    return answer\n\n# Пример использования\nquestion = \"Алиса жасы нешеде?\"\ncontext = \"Алиса 50 жаста ол робот\"\n\nanswer = ask_question(question, context)\nprint(\"Ответ:\", answer)\n","metadata":{"id":"TvlkjsUxmnVW","outputId":"e4c036a6-7cff-4394-fca9-0aea1d5267ae"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"Uwp4g0FOowcz"}},{"cell_type":"code","source":"small_dataset","metadata":{"id":"aJR-uajimneA","outputId":"6d9e0ad7-be81-4549-89fd-c32a9270591c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Вывод первых 100 записей из тренировочного набора\nfor idx, data in enumerate(small_dataset['train']):\n    print(f\"Запись {idx + 1}:\")\n    print(f\"Вопрос: {data['question']}\\n\")\n    print(f\"Контекст: {data['context']}\\n\")\n    print(f\"Ответы: {data['answers']}\\n\")\n    print(\"-\" * 80)\n","metadata":{"id":"LhE_ReCgmngO","outputId":"ffc04fcc-e3fd-42d2-b4ad-6499bb0fc550"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Импорт необходимых библиотек\nfrom transformers import RobertaTokenizerFast, RobertaForQuestionAnswering, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# Шаг 1: Загрузка датасета sKQuAD и выбор первых 1000 строк\ndataset = load_dataset(\"Kyrmasch/sKQuAD\")\ntrain_data = dataset[\"train\"].select(range(1000))\n\n# Шаг 2: Подготовка токенизатора и модели\nmodel_name = \"nur-dev/roberta-kaz-large\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_name)\nmodel = RobertaForQuestionAnswering.from_pretrained(model_name)\n\n# Шаг 3: Предобработка данных\ndef prepare_train_features(examples):\n    tokenized_examples = tokenizer(\n        examples[\"question\"], examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=384,\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    start_positions = []\n    end_positions = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        context = examples[\"context\"][sample_index]\n        answer = examples[\"answer\"][sample_index]\n\n        # Определение позиции ответа в контексте\n        answer_start = context.find(answer)\n        answer_end = answer_start + len(answer)\n\n        # Проверка, что ответ найден в контексте\n        if answer_start == -1:\n            start_positions.append(cls_index)\n            end_positions.append(cls_index)\n        else:\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):\n                start_positions.append(cls_index)\n                end_positions.append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:\n                    token_start_index += 1\n                start_positions.append(token_start_index - 1)\n\n                while offsets[token_end_index][1] >= answer_end:\n                    token_end_index -= 1\n                end_positions.append(token_end_index + 1)\n\n    tokenized_examples[\"start_positions\"] = start_positions\n    tokenized_examples[\"end_positions\"] = end_positions\n\n    return tokenized_examples\n\n# Применение функции предобработки\ntokenized_train_data = train_data.map(prepare_train_features, batched=True, remove_columns=train_data.column_names)\n\n# Шаг 4: Настройка Trainer и TrainingArguments с измененными гиперпараметрами\ntraining_args = TrainingArguments(\n    output_dir=\"./roberta-kaz-squad-improved\",\n    evaluation_strategy=\"no\",  # Отключение оценки\n    learning_rate=2e-5,  # Уменьшенная скорость обучения для более точного обучения\n    per_device_train_batch_size=4,  # Уменьшенный размер пакета для экономии памяти\n    gradient_accumulation_steps=4,  # Накопление градиентов, эффективно увеличивая размер пакета\n    num_train_epochs=5,  # Увеличенное количество эпох для лучшего обучения\n    weight_decay=0.1  # Усиленный weight decay для уменьшения переобучения\n)\n\n# Инициализация Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_data\n)\n\n# Шаг 5: Fine-tuning модели\ntrainer.train()\n\n# Шаг 6: Сохранение обученной модели\nmodel.save_pretrained(\"./roberta-kaz-squad-improved\")\ntokenizer.save_pretrained(\"./roberta-kaz-squad-improved\")\n","metadata":{"id":"K_A9U53mlHvH"},"outputs":[],"execution_count":null}]}